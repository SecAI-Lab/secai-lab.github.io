- title: "WebRR: A Forensic System for Replaying and Investigating Web-Based Attacks in The Modern Web"
  link: "https://www.usenix.org/system/files/usenixsecurity24-allen.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1qDnQenen3E0UZVOCW3djGYVj6mr4jiUL02NUREJGN9g/edit?usp=sharing"
  code_link: ""
  presenter: Yujeong
  conference: USENIX
  date: 2024.09.25
  pub_year: 2024
  discussion: "(Discussion) "

- title: "Can I Hear Your Face? Pervasive Attack on Voice Authentication Systems with a Single Face Image"
  link: "https://www.usenix.org/system/files/usenixsecurity24-jiang-nan.pdf"
  ppt_link: "https://for8821.synology.me:5001/d/s/104DzKxOnLNGeaWBOfgyYqS4Gi4SZcJa/8jNBK92x6DwadsBPx6MRxFk2e5ICD9Bb-cbNguYYzqQs"
  code_link: ""
  presenter: Minseok
  conference: USENIX
  date: 2024.09.11
  pub_year: 2024
  discussion: "(Discussion) This paper presents a novel deepfake attack called Foice. This attack generates a synthetic voice of a victim using just a single face image, without requiring any voice samples. The generated voice is realistic enough to fool commercial voice authentication systems, such as those used by WeChat, Microsoft Azure, and other platforms. The core idea behind Foice is to learn the partial correlation between face and voice features, and then combine these with random face-independent voice features generated from a Gaussian distribution. The attack's effectiveness was demonstrated through real-world experiments on several authentication systems and voice assistants, where it showed high success rates, indicating significant vulnerabilities in these systems. The study emphasizes the importance of new defenses against such attacks, as current systems lack sufficient protection against deepfake threats based solely on facial images."

- title: "Decoding the MITRE Engenuity ATT&CK Enterprise Evaluation: An Analysis of EDR Performance in Real-World Environments"
  link: "https://dl.acm.org/doi/pdf/10.1145/3634737.3645012"
  ppt_link: "https://docs.google.com/presentation/d/1sfIBva1gaq-rwEg_alsK-m0IHV-iBKdzcyGl-z-9Lr0/edit?usp=sharing"
  code_link: ""
  presenter: Shakhzod
  conference: ASIA CCS
  date: 2024.09.05
  pub_year: 2024
  discussion: "(Discussion) The paper provides a comprehensive analysis of the MITRE ATT&CK evaluations, aiming to address the limitations of the raw evaluation results. The paper's strengths lie in its introduction of new analysis methods, including whole-graph analysis and holistic assessments, to gain deeper insights into EDR system capabilities. The authors' reconstruction of attack scenarios and subsequent analysis shed light on EDR systems' attack reconstruction, behavior correlation, and overall detection performance. However, the paper could be improved by addressing the lack of access to crucial information like false positive alarm volume, response time, and raw data, which limits the scope of the analysis. Despite this limitation, the paper offers valuable contributions to the field by providing a systematic interpretation of MITRE ATT&CK evaluation results, aiding researchers, practitioners, and vendors in understanding and improving EDR systems."

- title: "Racing on the Negative Force: Efficient Vulnerability Root-Cause Analysis through Reinforcement Learning on Counterexamples"
  link: "https://www.usenix.org/system/files/usenixsecurity24-xu-dandan.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1l2Tb8lClYwJTgiKJELEmXUCT36a3PE-t/edit?usp=sharing&ouid=112194344922196039599&rtpof=true&sd=true"
  code_link: "https://github.com/0xdd96/Racing-code/tree/master"
  presenter: Suyeon
  conference: USENIX
  date: 2024.08.28
  pub_year: 2024
  discussion: "(Discussion) This paper presents RACING, a novel approach for efficient root cause analysis (RCA) in fuzzing. By leveraging counterexamples and reinforcement learning, RACING intelligently guides the fuzzing process to quickly identify the root cause of crashes. The experimental results demonstrate that RACING significantly outperforms the state-of-the-art technique, Aurora, in terms of both speed and accuracy. This work holds significant implications for improving the efficiency and effectiveness of vulnerability detection and analysis in software. However, limitations exist, such as the reliance on source code and the lack of support for compound predicates, which offer avenues for future research. Overall, RACING represents a promising advancement in automated RCA for fuzzing, with the potential to significantly impact the field of software security."

- title: "Data Coverage for Guided Fuzzing"
  link: "https://www.usenix.org/system/files/usenixsecurity24-wang-mingzhe.pdf"
  ppt_link: "https://secai.skku.edu/assets/paper-presentation/jiyong_DataCovFuzz_USENIX24.pdf"
  code_link: "https://github.com/THU-WingTecher/wingfuzz"
  presenter: Jiyong
  conference: USENIX
  date: 2024.08.28
  pub_year: 2024
  discussion: "(Discussion) Code coverage faces a major challenge in that it only reflects a small part of a program's structure, leaving some crucial program constructs uncovered. To address this issue, this work proposes data coverage for guided fuzzing - a technique that focuses on detecting novel constant data references and maximizing their coverage. Additionally, real-world fuzzing practices were optimized by classifying data access according to semantics and designing customized collection strategies. This is crucial because improper handling of constant data can significantly impact fuzzing throughput. To further improve fuzzing efficiency, novel storage and utilization techniques were developed. Finally, libFuzzer was enhanced with data coverage capabilities and submitted to Google's FuzzBench for evaluation. In this evaluation, the proposed approach outperformed many state-of-the-art fuzzers and achieved the best coverage score in the experiment. As a result, using this enhanced code coverage, 28 previously unknown bugs in OSS-Fuzz projects were discovered."
  
- title: "Post Hoc Explanations of Language Models Can Improve Language Models"
  link: "https://arxiv.org/pdf/2305.11426"
  ppt_link: "https://docs.google.com/presentation/d/1_2KVJPmfgwGfG90SxW2SivejuPYqtM2xpqqVR6GyCsk/edit#slide=id.p"
  code_link: ""
  presenter: Yujeong
  conference: NeurIPS
  date: 2024.08.21
  pub_year: 2023
  discussion: "(Discussion) LLMs demonstrate remarkable capabilities in various complex tasks. Recent research has delved into the possibility of enhancing the performance of these LLMs by incorporating human-annotated data. Unfortunately, this method is limited in scalability and underperforms in specific scenarios. This paper proposes a technique that can automatically generate natural language rationales using the output attribution scores that capture the influence of each feature on model predictions. The new framework, AMPLIFY, improves the prediction accuracy to about 10-25%, including those where prior approaches relying on human-annotated rationales fall short. A fundamental limitation of AMPLIFY is that it inherits the limitations of LLMs and Post hoc explanation methods. "

- title: "Beyond Memorization: The Challenge of Random Memory Access in Language Models"
  link: "https://arxiv.org/abs/2403.07805"
  ppt_link: "https://drive.google.com/file/d/1N5lHoFqZwlmH61lze12jDE9iCR7xtkei/view?usp=sharing"
  code_link: "https://github.com/sail-sg/lm-random-memory-access"
  presenter: Intae
  conference: ACL
  date: 2024.08.21
  pub_year: 2024
  discussion: "(Discussion) This paper explores the memory access patterns of language models (LMs), particularly focusing on the challenges they face with random access to memorized information. The authors demonstrate that while LMs can sequentially reproduce stored content effectively, they struggle with accessing information in random segments, especially in the middle of memorized sequences. To address this, the paper proposes two strategies that \"recitation\" (having the model first recall the content) and \"permutation\" (shuffling sentence order) and shows through experiments that these methods can significantly improve the model's performance. The study provides valuable insights into the limitations of memory access in LMs and suggests practical ways to enhance their performance in real-world applications. However, the research is limited to decoder-only models and does not explore larger models beyond 7 billion parameters, which could offer further insights. Additionally, experiments are conducted on a fixed-size text corpus, leaving questions about scalability to larger pretraining datasets. Despite these limitations, the paper makes an important contribution to understanding and improving memory access in language models."
  
- title: "On Large Language Models’ Resilience to Coercive Interrogation"
  link: "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a252/1WPcZ9B0jCg"
  ppt_link: "https://bit.ly/4fJN0cU"
  code_link: ""
  presenter: Minseok
  conference: S&P
  date: 2024.08.14
  pub_year: 2024
  discussion: "(Discussion) In this paper, a new weakness in Large Language Models (LLMs) is exposed that doesn’t rely on creating special prompts, known as jail-breaking. Instead, this method, called model interrogation, uses the fact that even when an LLM refuses to answer a harmful question, the damaging reply might still be hidden in the less obvious parts of its output. By accessing the list of probable responses (top-k token predictions) available in many open-source and commercial LLMs, a person with bad intentions can manipulate the model to reveal these harmful answers. They do this by choosing less likely words from the list at certain points in the response. This technique proves to be not only different but also more effective than traditional jail-breaking, with a 92% success rate compared to 62%, and it works 10 to 20 times faster. The harmful content brought out by this method is also of higher quality. Moreover, combining model interrogation with jail-breaking methods greatly improves results over using either technique alone. The study also shows that LLMs made for specific tasks like programming can still be tricked into giving harmful responses using this method."

- title: "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked"
  link: "https://arxiv.org/abs/2308.07308"
  ppt_link: "https://docs.google.com/presentation/d/1IaRYYq8H7n_TaPSZvcQ4vX6MoKCRS4uBQ1Tt2Fv4-Hw/edit?usp=sharing"
  code_link: "https://github.com/poloclub/llm-self-defense"
  presenter: Shakhzod
  conference: Arxiv
  date: 2024.08.14
  pub_year: 2024
  discussion: "(Discussion) The authors of the paper present a clever and straightforward approach to enhance the safety of large language models (LLMs). The idea is to have the LLM essentially double-check its own work for any potentially harmful content.  When the LLM receives a prompt that could lead to a harmful response, it generates the text as usual. But then, this response is passed to another LLM  that's been instructed to act as a filter. This filter LLM reads the generated text and makes a judgment call by evaluating \"harmfulness\". The beauty of this method lies in its simplicity - doesn't require any retraining or tweaking of the original LLM, also it doesn't involve any complex pre-processing of the input. They tested this \"LLM self-defense\" mechanism on two popular models, GPT-3.5 and LLaMA 2. In many cases, they were able to virtually eliminate the generation of harmful content."

- title: "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization"
  link: "https://arxiv.org/abs/2407.02524"
  ppt_link: "https://secai.skku.edu/assets/paper-presentation/jiyong_LLMCOMPILER_ARXIV.pdf"
  code_link: "https://huggingface.co/facebook"
  presenter: Jiyong
  conference: Arxiv
  date: 2024.08.07
  pub_year: 2024
  discussion: "(Discussion) This paper introduces LLM Compiler, a novel large language model designed for code optimization and compiler tasks. Building upon the Code Llama model, LLM Compiler is specifically trained to understand intermediate representations (IR) and assembly code, showing improved performance in code generation and compiler emulation tasks. The model demonstrates strong capabilities in handling compiler-related tasks and outperforms previous models in various benchmarks. However, the paper notes some limitations. The primary issue is the model's fixed input sequence length of 16k tokens, which restricts its ability to handle very large codebases effectively. Despite efforts to split large translation units, some remain too large for the model to process. Additionally, the accuracy of the model's outputs requires rigorous evaluation and verification to ensure correctness, as any suggested optimizations should be thoroughly tested. Despite these limitations, the paper makes significant contributions to the field of compiler optimization and provides a solid foundation for future research."

- title: "Unlearning Bias in Language Models by Partitioning Gradients"
  link: "https://aclanthology.org/2023.findings-acl.375.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1c4g1xazZW56AzWrCqAKcMOP2GARlfW9YL5xzDdjBqhk/edit#slide=id.p"
  code_link: "https://github.com/CharlesYu2000/PCGU-UnlearningBias"
  presenter: Yujeong
  conference: ACL
  date: 2024.07.29
  pub_year: 2023
  discussion: "(Discussion) The paper 'Unlearning Bias in Language Models by Partitioning Gradients' presents a novel technique, partitioned contrastive gradient unlearning (PCGU), to debias pretrained masked language models. PCGU selectively optimizes weights that contribute to biases by calculating gradients for contrastive sentence pairs. Evaluations using StereoSet and CrowS Pairs datasets demonstrate PCGU's effectiveness in reducing gender-profession bias with minimal impact on language modeling performance. Additionally, PCGU shows potential in mitigating biases across other domains like race and religion. However, it would have been better if there were additional experiments depending on the size of the language model. Also This technique highlights the need for ongoing research to develop robust strategies for addressing bias in language models."

- title: "ProPILE: Probing Privacy Leakage in Large Language Models"
  link: "https://arxiv.org/abs/2307.01881"
  ppt_link: "https://drive.google.com/file/d/1kqLFJMrElBKXLEI67QEohhRkpi_r3RZf/view?usp=sharing"
  code_link: ""
  presenter: Intae
  conference: NeurIPS
  date: 2024.07.24
  pub_year: 2024
  discussion: "(Discussion) ProPILE, a novel probing tool designed to assess the risk of Personally Identifiable Information (PII) leakage in Large Language Models (LLMs), demonstrates the feasibility of extracting PII through strategic prompt engineering. By utilizing black-box probing for data subjects and white-box probing for LLM service providers, ProPILE enables the assessment of PII leakage in models like OPT-1.3B trained on the Pile dataset. Experiments reveal that the likelihood of target PII generation increases with more specific prompts or additional linked PII details, while white-box probing with access to a limited subset of training data significantly amplifies this leakage potential. These findings underscore the effectiveness of ProPILE as an assessment tool and highlight the need for ongoing research and robust mitigation strategies to address privacy vulnerabilities in LLMs."

- title: "Improving Real-world Password Guessing Attacks via Bi-directional Transformers"
  link: "https://www.usenix.org/conference/usenixsecurity23/presentation/xu-ming"
  ppt_link: "https://drive.google.com/file/d/1O9FE6M3x5XVBqyubhEU8nq3ongwgtS-g/view?usp=sharing"
  code_link: "https://github.com/snow0011/PassBertStrengthMeter"
  presenter: Minseok
  conference: USENIX
  date: 2024.07.16
  pub_year: 2023
  discussion: "(Discussion) This paper proposes a bi-directional transformer-based guessing framework, referred to as PassBERT, which applies the pre-training/fine-tuning paradigm to password guessing attacks. First, a model pre-trained on the general password distribution was prepared, which was then fine-tuned on three specifically designed attack approaches. These methods reflect real-world attack scenarios and include: 1) conditional password guessing, which recovers the complete password given a partial one; 2) targeted password guessing, which compromises the password of a specific user using personal information; and 3) adaptive rule-based password guessing, which selects rules to generate rule-transformed password candidates. The experimental results show that the fine-tuned models can outperform state-of-the-art models by 14.53%, 21.82%, and 4.86% in the three attacks, respectively, demonstrating the effectiveness of bi-directional transformers on downstream guessing attacks. Furthermore, a hybrid password strength meter was proposed to mitigate the risks from these three types of attacks."

- title: "Semantic Ranking for Automated Adversarial Technique Annotation in Security Text"
  link: "https://dl.acm.org/doi/10.1145/3634737.3645000"
  ppt_link: "https://docs.google.com/presentation/d/1GY56E7gxjBpjBzh_3xWKjuGtpQwAAFe3YP9QbC8wkG0/edit?usp=sharing"
  code_link: "https://github.com/qcri/Text2TTP"
  presenter: Shakhzod
  conference: ASIA CCS
  date: 2024.07.12
  pub_year: 2024
  discussion: "(Discussion) This paper introduces an innovative multi-stage ranking system for extracting and annotating adversarial techniques from threat intelligence reports, leveraging language models fine-tuned for cybersecurity tasks. The system demonstrates significant improvements in accuracy and recall compared to previous methods, with enhanced performance on verbose datasets like MITRE ATT&CK Reports and WeLiveSecurity. The comprehensive approach, including testing across various datasets and the introduction of a public dataset with 6.6K annotations, strengthens the study's contributions to the field. However, the observed performance disparity across datasets highlights the need for further refinement to handle concise descriptions more effectively. Overall, the study advances automated threat technique annotation and provides a solid foundation for future cybersecurity research and development."

- title: "LogBERT: Log Anomaly Detection via BERT"
  link: "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534113"
  ppt_link: "https://docs.google.com/presentation/d/1KkdM8aM_V4SBxDhb4lfbn9WcOOp9gH88/edit?usp=sharing&ouid=112194344922196039599&rtpof=true&sd=true"
  code_link: "https://github.com/HelenGuohx/logbert"
  presenter: Suyeon
  conference: IJCNN
  date: 2024.07.04
  pub_year: 2021
  discussion: "(Discussion) Detecting anomalies within system logs is paramount to protecting the system from an attack or malfunction. Traditional methods employ regular expressions or machine learning models to identify anomalous events. These approaches depend on handcrafted features and are unable to capture temporal information. For example, malicious logs could be benign on their own, but the collection of these logs could be malicious. Recently, deep learning models such as recurrent neural networks (RNNs) have been widely used to evaluate these sequences and can capture temporal information. However, RNNs cannot encode contextual information in a bi-directional manner. This paper proposes a log anomaly detection model based on BERT. BERT can capture contextual information in a bi-directional manner making it suitable for this task. The model is evaluated on three datasets: Hadoop Distributed File System (HDFS), BlueGene/L Supercomputer System (BGL), and Thunderbird-mini dataset. The baseline models are Principal Component Analysis (PCA), One-Class SVM (OCSVM), IsolationForest (iForest), LogCluster, DeepLog, and LogAnomaly. Their proposed model demonstrates superior performance over all baseline state-of-the-art models in all datasets. However, the detailed process in training is not described. For example, BERT splits the training step into a pretraining and finetuning phase. However, LogBERT seems to only discuss the pretraining phase and have no mention about the fine tuning phase. As a result, it is not clear if the fine tuning phase is excluded or it is not mentioned."

- title: "Universal and Transferable Adversarial Attacks on Aligned Language Models"
  link: "https://arxiv.org/abs/2307.15043"
  ppt_link: "https://secai.skku.edu/assets/paper-presentation/jiyong_UniversalandTransferable_arxiv2023.pdf"
  code_link: "https://github.com/llm-attacks/llm-attacks"
  presenter: Jiyong
  conference: arxiv
  date: 2024.06.24
  pub_year: 2023
  discussion: "(Discussion) This paper proposes an attack method that causes aligned language models to generate undesirable behaviors. 
  Aligned language models refuse to respond to harmful queries.
  To enable these language models to respond to harmful queries, this paper attaches a suffix to the query. 
  This approach makes the LLM generate a positive response instead of refusing to answer.
  Positive responses to harmful queries were obtained from ChatGPT, Bard, Claude, LLaMA-2-Chat, Pythia, and Falcon, with a much higher success rate for GPT-based models. 
  The evaluation section only demonstrates the high success rate of the proposed attack method. 
  It would have been better if the paper included the limitations of this study and a model ablation study section."
 
- title: "Membership Inference via Backdooring"
  link: "https://www.ijcai.org/proceedings/2022/0532.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1jwipRPV6XKZBzVBntrMpTLbh6GcH_0SlkN70jBY0r5U/edit#slide=id.p1"
  code_link: "https://github.com/HongshengHu/membership-inference-via-backdooring"
  presenter: Yujeong
  conference: IJCAI
  date: 2024.06.17
  pub_year: 2022
  discussion: "(Discussion) The paper titled 'Membership Inference via Backdooring' presents a novel approach to addressing data privacy concerns in machine learning by introducing a method called Membership Inference via Backdooring (MIB). 
  The main contribution of MIB lies in its ability to mark a small number of data samples, which, when used by an unauthorized party to train a model, allows the data owner to later identify this misuse through black-box queries and statistical hypothesis testing. 
  However, several limitations and challenges accompany this promising approach. Firstly, the success of MIB hinges on the ability to inject backdoor triggers in a manner that remains undetectable by the unauthorized party. 
  While the paper discusses techniques to make the triggers imperceptible, there is an inherent risk that sophisticated adversaries could develop detection methods to identify and neutralize these backdoors. 
  Furthermore, the approach's reliance on statistical hypothesis testing to provide guarantees for inference results, while innovative, may still be vulnerable to model-specific variations and adversarial defenses. 
  However, further exploration is needed to assess the robustness of MIB across various types of datasets and models not covered in the experiments of the paper. 
  Additionally, it is regrettable that the examples from the various datasets experimented in Figure 5 were not also included."
  
- title: "llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations"
  link: "https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/"
  ppt_link: "https://drive.google.com/file/d/1y7YnZa1PRxe4Au0qOPsw92dvqk6vMn6G/view?usp=sharing"
  code_link: ""
  presenter: Intae
  conference: Meta
  date: 2024.05.27
  pub_year: 2023
  discussion: "(Discussion) The paper titled 'Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations' introduces Llama Guard, a model developed to enhance safety in human-AI interactions. 
  The model leverages the Llama2-7b architecture, fine-tuned to classify and mitigate safety risks in both user prompts and AI responses. 
  The core contribution is a comprehensive safety risk taxonomy that guides the classification process, encompassing categories like violence, hate speech, sexual content, and self-harm. 
  Llama Guard outperforms existing moderation tools on benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat. 
  The model supports customizable and adaptive use through zero-shot and few-shot learning, and its weights are publicly available for further research and development. However, the work has some limitations. 
  Llama Guard's common sense knowledge is constrained by its training data, which can lead to incorrect judgments. 
  Its performance in non-English languages is not guaranteed, and the quality of fine-tuning labels may not comprehensively cover all policy aspects, leading to subpar performance in some cases."

- title: "Humans vs. Machines in Malware Classification"
  link: "https://www.usenix.org/conference/usenixsecurity23/presentation/aonzo"
  ppt_link: "https://docs.google.com/presentation/d/1E7GtE7fmpe3vDWGTMoDnOOV5WDJGdcftWfWkmOfs660/edit?usp=sharing"
  code_link: ""
  presenter: Shakhzod
  conference: USENIX
  date: 2024.05.20
  pub_year: 2023
  discussion: "(Discussion) This study collects and analyzes data from human participants through malware classification games, clearly revealing the differences between human and machine learning models. 
  This is an original approach not seen in previous studies. However, the number of samples used in the experiment is limited to 20, which may be somewhat insufficient to represent the overall malware classification problem. 
  Nevertheless, their results provide practical insights that can be applied directly to training malware analysis experts and improving ML models. For example, they propose integrating dynamic behavior analysis of human experts into ML models. 
  This work provides important insights into how human experts and ML models classify malware, and suggests that a hybrid approach that incorporates human intuition and experience into ML models could be highly effective in future malware defense."

- title: "Anomaly Detection in Aerial Videos With Transformers"
  link: "https://ieeexplore.ieee.org/document/9854892"
  ppt_link: "https://drive.google.com/file/d/1GoRgFU5OZ4xQtBfWOHyUOEKuDLb59K6C/view?usp=sharing"
  code_link: "https://github.com/Jin-Pu/Drone-Anomaly"
  presenter: Suyeon
  conference: IEEE GRSS
  date: 2024.05.13
  pub_year: 2022
  discussion: "(Discussion) Let there be a video of a vehicle moving backward. An anomaly detection by frame alone would fail to capture any anomalies; instead, the model requires a sequence of frames to identify the anomaly. This paper proposes a Transformer-based anomaly detection in aerial videos from an Unmanned Aerial Vehicles (UAVs). By leveraging the Transformer encoder, they claim that the model can preserve spatiotemporal information.  The main contribution comprises an annotated dataset for realistic anomalous events, a benchmark for anomaly detection, and a baseline model ANDT. ANDT exhibits the best performance for certain scenes but does not do so for all scenes tested in the paper. Initial claims suggest that previous models do not preserve spatiotemporal information. However, previous models demonstrate decent performance on the provided dataset. In Figure 4, the MemAE model can infer the vehicle moving backward as an anomalous event, demonstrating its ability to preserve spatiotemporal information. Furthermore, the train test split in Table 1 is odd yet there is no mention why it was done in this way. For example, the test set is larger than the train set in the Bike roundabout scene."

- title: "Ahoy SAILR! There is No Need to DREAM of C: A Compiler-Aware Structuring Algorithm for Binary Decompilation"
  link: "https://www.usenix.org/conference/usenixsecurity24/presentation/basque"
  ppt_link: "https://secai.skku.edu/assets/paper-presentation/jiyong_SAILR_USENIX24.pdf"
  code_link: "https://github.com/mahaloz/sailr-eval"
  presenter: Jiyong
  conference: USENIX
  date: 2024.05.07
  pub_year: 2024
  discussion: "(Discussion) In many ways, the compilation process is destructive. High-level constructs, variable names, and comments present in the source code are often lost. Nevertheless, decompilers aim to retrieve the source from a binary. Decompilation results vary between decompilers. So, what is a good decompilation? One could argue that fewer goto instructions indicate good decompilation. Because multiple gotos signify the decompiler failed to structure the control flow. This paper argues that good decompilation is similar to the source code and points out that 3,754 goto instructions are present in the Linux kernel. Since a goto instruction could be intentional by the developer, they argue we should separate intended gotos from unintended gotos. The authors investigate the gcc compiler to identify the transformation passes that cause unintended gotos. Afterward, they propose a structuring algorithm that deoptimizes code to remove unintended gotos while preserving intended ones. SAILR, the proposed structuring algorithm, is implemented on the angr decompiler. SAILR demonstrates decent performence against state-of-the-art decompilers. The paper evaluates SAILR with 7,355 functions from 26 popular Debian packages. However, the number of functions is oddly low, considering the number of packages used."

- title: "DETECTING PRETRAINING DATA FROM LARGE LANGUAGE MODELS"
  link: "https://arxiv.org/pdf/2310.16789"
  ppt_link: "https://docs.google.com/presentation/d/1xjVmvLr7tYqUz17G5NAGJA8m8B_ep3Fm/edit?usp=sharing&ouid=107878723841384409882&rtpof=true&sd=true"
  code_link: "https://swj0419.github.io/detect-pretrain.github.io/"
  presenter: Yujeong
  conference: ICLR
  date: 2024.04.29
  pub_year: 2024
  discussion: "(Discussion) This paper proposes the Membership Inference method 'MIN-K% PROB' in LLM(Large Language Model). 
  The problem presented in this paper is the challenge of detecting pretraining data. 
  This is because LLM developers do not open the data used for pertaining, and since pretraining involves training on data instances once at a time, it makes detection even more difficult.
  Hence, this paper assumes that the Membership Inference Detector cannot know the distribution of pretraining data.
  This means that there is no Reference Model (e.g., Shadow Model) used in Membership Inference techniques.
  Consequently, this paper proposes the Reference-free Method, MIN-K% PROB.
  The 'Min-K% PROB' method selects outlier tokens with low token probability to create a set, and then uses the average of this set as the threshold for inferring between members and non-members.
  This method seems to be efficient as it infers membership based on the token-level probability during the pretraining.
  Additionally, using this method as an evaluation for Unlearning techniques, as demonstrated in the case study, would be beneficial."

- title: "You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content"
  link: "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a061/1RjEayUL076"
  ppt_link: "https://drive.google.com/file/d/1bRcOLeW2NlZfgnXxwwlwu6mRWjvWaO1P/view?usp=drive_link"
  code_link: "https://github.com/xinleihe/toxic-prompt"
  presenter: Intae
  conference: S&P
  date: 2024.04.22
  pub_year: 2024
  discussion: "(Discussion) The paper investigates the application of prompt learning with Large Language Models (LLMs) such as GPT-3 and T5 to address the issue of toxic online content. It focuses on three primary tasks: toxicity classification, toxic span detection, and detoxification, showing that prompt learning can perform as well as or even better than traditional models specifically trained for these tasks. Notably, this approach achieves a 10% improvement in toxicity classification and significantly lowers toxicity scores in detoxification tasks while maintaining the semantic integrity of the content. The study is distinguished by its innovative use of prompt tuning for managing toxic content and its thorough evaluation across five model architectures and eight different datasets, which verifies the method's effectiveness and efficiency. However, despite these strengths, the approach's dependency on the quality of datasets and its ability to generalize to unseen toxic content remain potential weaknesses. Furthermore, the complexity involved in designing effective prompts and the possible misuse of the techniques are also concerns."

- title: "ANUBIS: a provenance graph-based framework for advanced persistent threat detection"
  link: "https://dl.acm.org/doi/abs/10.1145/3477314.3507097"
  ppt_link: "https://docs.google.com/presentation/d/1HbiVu05FGPdXl_mRfIeUiZjFIEBwC9uDd9jaiEAOF40/edit?usp=sharing"
  code_link: ""
  presenter: Shakhzod
  conference: SAC
  date: 2024.04.15
  pub_year: 2022
  discussion: "(Discussion) This paper proposes 'ANUBIS', a provenance graph-based supervised APT detection framework. ANUBIS is a method leveraging a BNN(Bayesian Neural Network) to overcome limitations in current APT detection methods using provenance graphs. 
  The authors hypothesize that an attacker cannot breach the probability graph used to train ANUBIS, nor does it violate the event logging mechanism of the host system. 
  These assumptions provide a strong premise for security scenarios, but they have limitations that do not take into account the practical considerations. 
  However, this paper is significant because it has demonstrated that it is effective in predicting the nature of the activity by introducing a new graph neighbor encoding method."

- title: "The Circle Of Life: A Large-Scale Study of The IoT Malware Lifecycle"
  link: "https://www.usenix.org/system/files/sec21fall-alrawi-circle.pdf"
  ppt_link: "https://drive.google.com/file/d/1NGZ63h-rqPJo0bwrYvog_ti1cohR3hc0/view?usp=drive_link"
  code_link: "https://badthings.info/"
  presenter: Suyeon
  conference: USENIX
  date: 2024.04.08
  pub_year: 2021
  discussion: "(Discussion) This paper is a measurement study on the IoT Malware Lifecycle. It collects around 166K Linux-based IoT malware samples collected over a year to measure the characteristics of IoT Malware. The paper concludes with some characteristics that differentiate IoT malware from traditional malware, such as most IoT malware being a variant of the Mirai botnet. However, there does not seem to be unexpected findings. A slight complaint is that figures within the paper had unclear captions. Although the paper explains the concepts, some figures are difficult to understand."

- title: "How Machine Learning Is Solving the Binary Function Similarity Problem"
  link: "https://www.usenix.org/conference/usenixsecurity22/presentation/marcelli"
  ppt_link: "https://secai.skku.edu/assets/paper-presentation/jiyong_HowMachineLearn_USENIX22.pdf"
  code_link: "https://github.com/Cisco-Talos/binary_function_similarity"
  presenter: Jiyong
  conference: USENIX
  date: 2024.04.01
  pub_year: 2022
  discussion: "(Discussion) In this paper, a dataset composition for testing the latest BCSD (Binary Code Similarity Detection) techniques is proposed, and analysis of test results for various BCSD models, including the latest GNN-based BCSD and Embedding-based BCSD, has been performed. However, in the evaluation section, it is necessary to divide the results for similarity and similarity lacking into separate tables to enhance clarity. Additionally, there is a lack of explanation for the many abbreviations used in the dataset composition, making it difficult to understand."

- title: "Quark: Controllable Text Generation with Reinforced [Un]learning"
  link: "https://proceedings.neurips.cc/paper_files/paper/2022/file/b125999bde7e80910cbdbd323087df8f-Paper-Conference.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1dV-zasDjviDGyWy2qNTVgQGs4zMYPWv2gjZnluq5wtI/edit?usp=sharing"
  code_link: "https://github.com/GXimingLu/Quark"
  presenter: Yujeong
  conference: NeurIPS
  date: 2024.03.25
  pub_year: 2022
  discussion: "(Discussion) The paper discussion focuses on an innovative approach to optimizing the reward function of a Reinforcement Learning (RL) model to mitigate undesired behaviors. 
  The authors detail a three-stage algorithm comprising exploration, quantization, and learning, each critical to the model's development. 
  Notably, the paper demonstrates the model's effectiveness through three distinct evaluations, addressing the reduction of toxicity, improvement over negative baselines, and minimization of repetitive actions. 
  This structured evaluation underscores the model's capability to unlearn specific unwanted behaviors, which outperforms both baselines and state-of-the-art RL methods.\n  
  \n In this paper, the focus is on defining three undesirable behaviors and proposing three methods (e.g. reward function) to forget each behavior. 
  It seems inefficient to train a model separately for each behavior."

- title: "A Comprehensive Detection Method for the Lateral Movement Stage of APT Attacks"
  link: "https://ieeexplore.ieee.org/abstract/document/10273652"
  ppt_link: "https://docs.google.com/presentation/d/14hGmxlQ8qmIxAjBmY01_yPBjiL72VUp6xqJJz8Tef3o/edit?usp=drivesdk"
  code_link: ""
  presenter: Shakhzod
  conference: IEEE IoT-J
  date: 2024.03.19
  pub_year: 2023
  discussion: "(Discussion) This paper designs a multidimensional detection framework to detect lateral movement behavior against APT attacks in an intranet environment based on the SMB protocol. 
  However, contrary to this purpose, the experimental results are unfortunate in that the purpose and results differ because they are about how well malware has been classified."

- title: "Anomaly detection in a forensic timeline with deep autoencoders"
  link: "https://www.sciencedirect.com/science/article/abs/pii/S2214212621002076"
  ppt_link: "https://drive.google.com/file/d/1Ndma6vyN2FouuMMHTJIBRsRJ88VupIPm/view?usp=drive_link"
  code_link: ""
  presenter: Suyeon
  conference: JISA
  date: 2024.03.11
  pub_year: 2021
  discussion: "(Discussion) Systems generate logs to record internal events. These logs are used after a cyber incident for forensic investigation. Unfortunately, the system generates numerous logs during its runtime and majority of the analysis is manual. This paper proposes a deep autoencoder for anomaly detection to assist analyzers by highlighting anomalous events in the Linux kernal system logs.

Anomaly detection is a common application for autoencoders. It is utilized to analyze network traffic, logs, etc. It is difficult to identify what is different from previous work and this paper. Furthermore, the proposed model is evaluated with old dataset against ML methods such as SVM. It would have been preferable to see the model’s performance on up-to-date dataset against state-of-the-art models."

- title: "Binary Function Clone Search in the Presence of Code Obfuscation and Optimization over Multi-CPU Architectures"
  link: "https://dl.acm.org/doi/10.1145/3579856.3582818"
  ppt_link: "https://secai.skku.edu/assets/paper-presentation/jiyong_BinaryFunctionClone_ASIACCS23.pdf"
  code_link: ""
  presenter: Jiyong
  conference: ASIA CCS
  date: 2024.03.06
  pub_year: 2023
  discussion: "(Discussion) There are efficient ways to represent binary functions using a tokenizer (e.g., BPE) in assembly language. I'm curious about why the suggestion is to divide them into seven features like Vex-IR instructions, LibcCalls, Constants, etc., and create specific tokenizers for each feature to generate one-hot vectors. I also wonder if this approach is genuinely effective. The explanation about whether this method of tokenization has a positive impact on BCSD (Binary Code Similarity Detection) performance seems insufficient."

- title: "Learning to Unlearn: Instance-wise Unlearning for Pre-trained Classifiers"
  link: "https://arxiv.org/pdf/2301.11578.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1Td2WBmxQGBCHTaJPtb3L4fSdyHV_40ibzjugTEadyRg/edit?usp=sharing"
  code_link: ""
  presenter: Yujeong
  conference: AAAI
  date: 2024.02.28
  pub_year: 2024
  discussion: "(Discussion) The paper utilizes adversarial examples to retain the original decision boundary during unlearning. This approach 
is intriguing yet it raises questions as to what it means to “forget” a data point. The paper states that 
misclassification signifies forgetting yet “Right to be Forgotten” seems to imply the removal of a 
certain training data."

- title: "From Grim Reality to Practical Solution: Malware Classification in Real-World Noise"
  link: "https://ieeexplore.ieee.org/document/10179453"
  ppt_link: "https://secai.skku.edu/assets/paper-presentation/jiyong_FromGrimReality_SP23.pdf"
  code_link: "https://github.com/nuwuxian/morse"
  presenter: Jiyong
  conference: S&P
  date: 2024.02.20
  pub_year: 2023
  discussion: "Malware datasets inevitably contain incorrect labels due to the shortage of expertise and experience needed for sample labeling. Previous research demonstrated that a training dataset with incorrectly labeled samples would result in inaccurate model learning. To address this problem, researchers have proposed various noise learning methods to offset the impact of incorrectly labeled samples, and in image recognition and text mining applications, these methods demonstrated great success. In this work, we apply both representative and state-of-the-art noise learning methods to real-world malware classification tasks. We surprisingly observe that none of the existing methods could minimize incorrect labels’ impact. Through a carefully designed experiment, we discover that the inefficacy mainly results from extreme data imbalance and the high percentage of incorrectly labeled data samples. As such, we further propose a new noise learning method and name it after MORSE. Unlike existing methods, MORSE customizes and extends a state-of-the-art semi-supervised learning technique. It takes possibly incorrectly labeled data as unlabeled data and thus avoids their potential negative impact on model learning. In MORSE, we also integrate a sample re-weighting method that balances the training data usage in the model learning and thus handles the data imbalance challenge. We evaluate MORSE on both our synthesized and real-world datasets. We show that MORSE could significantly outperform existing noise learning methods and minimize the impact of incorrectly labeled data."

- title: "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"
  link: "https://arxiv.org/pdf/2109.00859.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1amblX-PbInT9vbEY1g8_tH1BNX60YOXAEVaNnCV6p4s/edit?usp=sharing"
  code_link: "https://github.com/salesforce/CodeT5"
  presenter: Yujeong
  conference: EMNLP
  date: 2024.02.12
  pub_year: 2021
  discussion: "(Abstract) Pre-trained models for Natural Languages
(NL) like BERT and GPT have been recently
shown to transfer well to Programming Languages (PL) and largely benefit a broad set of
code-related tasks. Despite their success, most
current methods either rely on an encoder-only
(or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks
or process the code snippet in the same way
as NL, neglecting the special characteristics of
PL such as token types. We present CodeT5,
a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned
identifiers. Our model employs a unified
framework to seamlessly support both code understanding and generation tasks and allows
for multi-task learning. Besides, we propose a
novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when
they are masked. Furthermore, we propose to
exploit the user-written code comments with a
bimodal dual generation task for better NL-PL
alignment. Comprehensive experiments show
that CodeT5 significantly outperforms prior
methods on understanding tasks such as code
defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture
semantic information from code. Our code
and pre-trained models are released at https:
//github.com/salesforce/CodeT5."

- title: "InCoder: A Generative Model for Code Infilling and Synthesis"
  link: "https://arxiv.org/pdf/2204.05999.pdf"
  ppt_link: "https://secai.skku.edu/assets/paper-presentation/jiyong_InCoder_ICLR23.pdf"
  code_link: "https://github.com/dpfried/incoder"
  presenter: Jiyong
  conference: EMNLP
  date: 2024.02.05
  pub_year: 2021
  discussion: "(Abstract) Code is seldom written in a single left-to-right pass and is instead repeatedly edited
and refined. We introduce INCODER, a unified generative model that can perform
program synthesis (via left-to-right generation) as well as editing (via masking
and infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and
moved to the end of each file, allowing code infilling with bidirectional context.
Our model is the first large generative code model that is able to infill arbitrary regions of code, which we evaluate in a zero-shot setting on challenging tasks such
as type inference, comment generation, and variable re-naming. We find that the
ability to condition on bidirectional context substantially improves performance
on these tasks, while still performing comparably on standard program synthesis
benchmarks in comparison to left-to-right only models pretrained at similar scale.
Our models and code are publicly released."

- title: "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"
  link: "https://arxiv.org/pdf/2207.01780.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1PieMht5fRiNtdxh7IMV6wNbVqLtsuJD3ho3a6w9-Oac/edit#slide=id.p1"
  code_link: "https://github.com/salesforce/CodeRL"
  presenter: Yujeong
  conference: NeurIPS
  date: 2024.01.22
  pub_year: 2022
  discussion: "(Abstract) Program synthesis or code generation aims to generate a program that satisfies a
problem specification. Recent approaches using large-scale pretrained language
models (LMs) have shown promising results, yet they have some critical limitations.
In particular, they often follow a standard supervised fine-tuning procedure to train a
code generation model only from the pairs of natural-language problem descriptions
and ground-truth programs. Such paradigm largely ignores some important but
potentially useful signals in the problem specification such as unit tests, which
thus often results in poor performance when solving complex unseen coding tasks.
To address the limitations, we propose “CodeRL”, a new framework for program
synthesis tasks through pretrained LMs and deep reinforcement learning (RL).
Specifically, during training, we treat the code-generating LM as an actor network,
and introduce a critic network that is trained to predict the functional correctness
of generated programs and provide dense feedback signals to the actor. During
inference, we introduce a new generation procedure with a critical sampling strategy
that allows a model to automatically regenerate programs based on feedback from
example unit tests and critic scores. For the model backbones, we extended the
encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger
model sizes, and better pretraining data. Our method not only achieves new SOTA
results on the challenging APPS benchmark, but also shows strong zero-shot
transfer capability with new SOTA results on the simpler MBPP benchmark."

- title: Ground Truth for Binary Disassembly is Not Easy
  link: "https://www.usenix.org/system/files/sec22-pang-chengbin.pdf"
  ppt_link: "https://drive.google.com/file/d/1BvH1VV88AKSkK5-WXDuqBE3CUMeeJEb3/view?usp=sharing"
  code_link: ""
  presenter: Jiyong
  conference: USENIX
  date: 2024.01.04
  pub_year: 2022
  discussion: "(Abstract) Modern disassembly tools often rely on empirical evaluations
to validate their performance and discover their limitations,
thus promoting long-term evolvement. To support the empirical evaluation, a foundation is the right approach to collect the
ground truth knowledge. However, there has been no unanimous agreement on the approach we should use. Most users
pick an approach based on their experience or will, regardless
of the properties that the approach presents.
In this paper, we perform a study on the approaches to
building the ground truth for binary disassembly, aiming to
shed light on the right way for the future. We first provide
a taxonomy of the approaches used by past research, which
unveils five major mechanisms behind those approaches. Following the taxonomy, we summarize the properties of the
five mechanisms from two perspectives: (i) the coverage and
precision of the ground truth produced by the mechanisms
and (ii) the applicable scope of the mechanisms (e.g., what
disassembly tasks and what types of binaries are supported).
The summarization, accompanied by quantitative evaluations,
illustrates that many mechanisms are ill-suited to support the
generation of disassembly ground truth. The mechanism best
serving today’s need is to trace the compiling process of the
target binaries to collect the ground truth information.
Observing that the existing tool to trace the compiling
process can still miss ground truth results and can only handle
x86/x64 binaries, we extend the tool to avoid overlooking
those results and support ARM32/AArch64/MIPS32/MIPS64
binaries. We envision that our extension will make the tool a
better foundation to enable universal, standard ground truth
for binary disassembly."

- title: Black-box Attacks Against Neural Binary Function Detection
  link: "https://arxiv.org/pdf/2208.11667.pdf"
  ppt_link: "https://docs.google.com/presentation/d/1-hVG6pd7kFOkQaHFIRfCWYISzBuVYyd1QcRixXiEf44/edit?usp=sharing"
  code_link: ""
  presenter: Nozima
  conference: RAID
  date: 2023.11.23
  pub_year: 2023
  discussion: "(Abstract) Binary analyses based on deep neural networks (DNNs), or neural
binary analyses (NBAs), have become a hotly researched topic in
recent years. DNNs have been wildly successful at pushing the
performance and accuracy envelopes in the natural language and
image processing domains. Thus, DNNs are highly promising for
solving binary analysis problems that are hard due to a lack of
complete information resulting from the lossy compilation process.
Despite this promise, it is unclear that the prevailing strategy of
repurposing embeddings and model architectures originally developed for other problem domains is sound given the adversarial
contexts under which binary analysis often operates.
In this paper, we empirically demonstrate that the current state of
the art in neural function boundary detection is vulnerable to both
inadvertent and deliberate adversarial attacks. We proceed from the
insight that current generation NBAs are built upon embeddings
and model architectures intended to solve syntactic problems. We
devise a simple, reproducible, and scalable black-box methodology for exploring the space of inadvertent attacks – instruction
sequences that could be emitted by common compiler toolchains
and configurations – that exploits this syntactic design focus. We
then show that these inadvertent misclassifications can be exploited
by an attacker, serving as the basis for a highly effective black-box
adversarial example generation process. We evaluate this methodology against two state-of-the-art neural function boundary detectors:
XDA and DeepDi. We conclude with an analysis of the evaluation
data and recommendations for ho"

- title: "SelectiveTaint: Efficient Data Flow Tracking With Static Binary Rewriting"
  link: "https://www.usenix.org/system/files/sec21-chen-sanchuan.pdf"
  ppt_link: "https://drive.google.com/file/d/1MKIYdXi0rUznXA7gyRL9GEHowHMNj-Sv/view?usp=sharing"
  code_link: ""
  presenter: Jiyong
  conference: USENIX
  date: 2023.10.05
  pub_year: 2021
  discussion: "(Abstract) Taint analysis has been widely used in many security applications such as exploit detection, information flow tracking, malware analysis, and protocol reverse engineering. State-of-theart taint analysis tools are usually built atop dynamic binary
instrumentation, which instruments at every possible instruction, and rely on runtime information to decide whether a particular instruction involves taint or not, thereby usually having
high performance overhead. This paper presents SELECTIVETAINT, an efficient selective taint analysis framework for binary executables. The key idea is to selectively instrument the
instructions involving taint analysis using static binary rewriting instead of dynamic binary instrumentation. At a high level,
SELECTIVETAINT statically scans taint sources of interest in
the binary code, leverages value set analysis to conservatively
determine whether an instruction operand needs to be tainted
or not, and then selectively taints the instructions of interest.
We have implemented SELECTIVETAINT and evaluated it
with a set of binary programs including 16 coreutils (focusing
on file I/O) and five network daemon programs (focusing
on network I/O) such as nginx web server. Our evaluation
results show that the binaries statically instrumented by SELECTIVETAINT has superior performance compared to the
state-of-the-art dynamic taint analysis frameworks (e.g., 1.7x
faster than that of libdft)."
